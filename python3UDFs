#! python3
"""
User-defined functions in python3
"""

from collections import deque

from datetime import datetime, date, timedelta
from pytz import timezone
import calendar

import numpy as np
import pandas as pd

from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Border, Side, Alignment, Font

import matplotlib
import matplotlib.pyplot as plt
plt.style.use('ggplot')

from collections import OrderedDict
from collections import abc

import glob
import os
import re
import shutil
import sys
import teradata
import timeit
import smtplib
import fnmatch

import json
from cassandra.auth import PlainTextAuthProvider
from cassandra.cluster import Cluster
from cassandra.query import SimpleStatement
from cassandra.query import BatchStatement
#from cassandra.query import ValueSequence
#from cassandra.query import dict_factory
from tqdm import tqdm
import requests

from multiprocessing import Process
from multiprocessing import Pool
from subprocess import Popen, list2cmdline
from itertools import repeat
from sys import platform

todayMonthVal = '{0:%b}'.format(date.today())
todayYearVal  = '{0:%Y}'.format(date.today())
#Example: datetime.strftime(date(2012, 9, 28) - timedelta(days = 1), '%m-%d-%Y')
yesterdayFormattedDate   = datetime.strftime(datetime.today() - timedelta(days = 1), '%m-%d-%Y')
todayFormattedDate       = datetime.strftime(datetime.today(), '%m-%d-%Y')
formattedPrevMonth       = '{0:%B}'.format(datetime.strptime(todayFormattedDate, '%m-%d-%Y').date().replace(day = 1) - timedelta(days = 1))
formattedPrevMonthYear   = '{0:%Y}'.format(datetime.strptime(todayFormattedDate, '%m-%d-%Y').date().replace(day = 1) - timedelta(days = 1))
formattedMonth           = '{0:%B}'.format(datetime.strptime(todayFormattedDate, '%m-%d-%Y').date())
formattedMonthYear       = '{0:%Y}'.format(datetime.strptime(todayFormattedDate, '%m-%d-%Y').date())

######################## Start of function definitions ########################

def pandas_factory(colnames, rows):
    return pd.DataFrame(rows,
                        columns = colnames
                        );

def authorizeGk(debug_flag):
    auth_provider = PlainTextAuthProvider(username = 'gatekeeper_app',
                                          password = 'app_gatekeeper'
                                          )
    
    if debug_flag == 1:
        print('Connecting to production keyspace in Cassandra DB.');
    # Production keyspace
    cluster = Cluster(['<IP address>'],
                      port = <port number>,
                      auth_provider = auth_provider
                     )
    session = cluster.connect('gk_audit')
    session.row_factory = pandas_factory
    session.default_fetch_size = None
    return session;

def downloadIncidentsFromCassandraDb(debug_flag):
    if debug_flag == 1:
        print('### Start: downloadIncidentsFromCassandraDb function ###');
    cassandraSession = authorizeGk(debug_flag);
    cassandraDbQuery = SimpleStatement("SELECT <column 1>, \
                                               <column 2>, \
                                               ... \
                                               FROM <table name>",
                                               fetch_size = 50000
                                       )
    if debug_flag == 1:
        print('Fetching query from Cassandra DB.');
    queryResults = cassandraSession.execute(cassandraDbQuery,
                                            timeout = None
                                            )
    queryResultsDf = queryResults._current_rows
    if debug_flag == 1:
        print(str(len(queryResultsDf)) + ' rows were fetched.\n')
        print('### End: downloadIncidentsFromCassandraDb function ###');
    return queryResultsDf;

def downloadIncidentsFromCassandraDbCanada(debug_flag):
    if debug_flag == 1:
        print('### Start: downloadIncidentsFromCassandraDbCanada function ###');
    cassandraSession = authorizeGk(debug_flag);
    cassandraDbQuery = SimpleStatement("SELECT * FROM <table name>",
                                        fetch_size = 50000
                                       )
    if debug_flag == 1:
        print('Fetching query from Cassandra DB.');
    queryResults = cassandraSession.execute(cassandraDbQuery,
                                            timeout = None
                                            )
    queryResultsDf = queryResults._current_rows
    if debug_flag == 1:
        print(str(len(queryResultsDf)) + ' rows were fetched.\n')
        print('### End: downloadIncidentsFromCassandraDbCanada function ###');
    return queryResultsDf;

def createDfForUploadingToTd(debug_flag, queryResultsDf, createItemsTbl):
    if debug_flag == 1:
        print('### Start: createDfForUploadingToTd ###')
    dfLen    = int(len(queryResultsDf) / 7)
    dfModLen = int(len(queryResultsDf) % 7)
    queryResults1Df         = queryResultsDf[(dfLen * 0): (dfLen * 1)]
    queryResults1Df.index   = range(len(queryResults1Df))
    queryResults2Df         = queryResultsDf[(dfLen * 1): (dfLen * 2)]
    queryResults2Df.index   = range(len(queryResults2Df))
    queryResults3Df         = queryResultsDf[(dfLen * 2): (dfLen * 3)]
    queryResults3Df.index   = range(len(queryResults3Df))
    queryResults4Df         = queryResultsDf[(dfLen * 3): (dfLen * 4)]
    queryResults4Df.index   = range(len(queryResults4Df))
    queryResults5Df         = queryResultsDf[(dfLen * 4): (dfLen * 5)]
    queryResults5Df.index   = range(len(queryResults5Df))
    queryResults6Df         = queryResultsDf[(dfLen * 5): (dfLen * 6)]
    queryResults6Df.index   = range(len(queryResults6Df))
    queryResults7Df         = queryResultsDf[(dfLen * 6): (dfLen * 7)]
    queryResults7Df.index   = range(len(queryResults7Df))
    if dfModLen > 0:
        queryResults8Df  = queryResultsDf[(dfLen * 7): len(queryResultsDf)];
        queryResults8Df.index = range(len(queryResults8Df))
    else:
        queryResults8Df = pd.DataFrame();
    with Pool(8) as proc1:
        if createItemsTbl == 1:
            resultsDf = proc1.map(createItemsDf,
                                  [(debug_flag, queryResults1Df),
                                   (debug_flag, queryResults2Df),
                                   (debug_flag, queryResults3Df),
                                   (debug_flag, queryResults4Df),
                                   (debug_flag, queryResults5Df),
                                   (debug_flag, queryResults6Df),
                                   (debug_flag, queryResults7Df),
                                   (debug_flag, queryResults8Df)
                                   ]
                                  )
        else:
            resultsDf = proc1.map(createContactsDf,
                                  [(debug_flag, queryResults1Df),
                                   (debug_flag, queryResults2Df),
                                   (debug_flag, queryResults3Df),
                                   (debug_flag, queryResults4Df),
                                   (debug_flag, queryResults5Df),
                                   (debug_flag, queryResults6Df),
                                   (debug_flag, queryResults7Df),
                                   (debug_flag, queryResults8Df)
                                   ]
                                  )
    items8Df = resultsDf.pop()
    items7Df = resultsDf.pop()
    items6Df = resultsDf.pop()
    items5Df = resultsDf.pop()
    items4Df = resultsDf.pop()
    items3Df = resultsDf.pop()
    items2Df = resultsDf.pop()
    items1Df = resultsDf.pop()
    
    if debug_flag == 1:
        print('Completed creating individual dataframes')
    dfForUploadingToTd = pd.concat([items1Df,
                                    items2Df,
                                    items3Df,
                                    items4Df,
                                    items5Df,
                                    items6Df,
                                    items7Df,
                                    items8Df
                                    ],
                                   axis = 0
                                   )
    if debug_flag == 1:
        print('### End: createDfForUploadingToTd ###')
    return dfForUploadingToTd;

#def dictToDf(dictKeyValPairs):
#    dictToPdDf = pd.DataFrame(dictKeyValPairs.items(),
#                              columns = dictKeyValPairs.keys())
#    dictToPdDf.set_index(0,
#                         inplace = True
#                         )
#    return dictToPdDf;

#def unravelNestedDict(dictKeyValuePair):
#    for keyVal, valueVal in dictKeyValuePair.items():
#        if isinstance(valueVal, dict):
#            unravelNestedDict(valueVal);
#        elif isinstance(valueVal, list):
#            for listIndex, listVal in enumerate(valueVal):
#                unravelNestedDict(listVal);
#        else:
#            print("{0} : {1}".format(keyVal, valueVal));

#def unravelNestedDict(dictKeyValuePair):
#    thisList = dictKeyValuePair.items() if isinstance(dictKeyValuePair, dict) else enumerate(dictKeyValuePair)
#    for keyVal, valueVal in thisList:
#        if isinstance(valueVal, dict) or isinstance(valueVal, list):
#            unravelNestedDict(valueVal);
#        else:
#            if keyVal != None:
#                print("{0} : {1}".format(keyVal, valueVal));
#
#def unravelNestedDict(dictKeyValuePair, pdDf):
#    thisList = pd.io.json.json_normalize(json.loads(dictKeyValuePair)) if isinstance(dictKeyValuePair, dict) else enumerate(dictKeyValuePair)
#    for keyVal, valueVal in thisList:
#        if isinstance(valueVal, dict) or isinstance(valueVal, list):
#            unravelNestedDict(valueVal);
#        else:
#            pdDf = pd.concat([pd.io.json.json_normalize(json.loads(valueVal)),
#                              pdDf
#                              ],
#                             axis = 1
#                             )
#            return pdDf;

#def createDfFromCdbDf(origDf):
#    temp1Df = pd.DataFrame()
#    temp2Df = pd.DataFrame()
#    print('Length of Df = ' + str(len(origDf)))
#    for index, rows in origDf.iterrows():
#        print('Index = ' + str(index))
#        temp1Df = pd.concat([origDf[index : index + 1].drop(['request'], axis = 1), pd.io.json.json_normalize(json.loads(origDf['request'][index]))], axis = 1)
#        print('temp1Df.index = ' + str(temp1Df.index))
#        temp2Df = pd.concat([temp2Df, temp1Df], axis = 0, ignore_index = True)
#        print('temp2Df.index = ' + str(temp2Df.index))
#    return temp2Df[:-1];

def deleteRowsFromCassandra(argVals):
    debug_flag, argVal1 = argVals
    if debug_flag == 1:
        print('### Start: deleteRowsFromCassandra ###');
    cassandraSession = authorizeGk(debug_flag);
    batch = BatchStatement()
    for indexVal, rowVal in argVal1.iterrows():
        <attribute 1>  = argVal1Df['<attribute1>'][indexVal]
        <attribute 2>  = int(argVal1Df['<attribute 2>'][indexVal])
        selectQueryVal = "DELETE FROM <table name> WHERE \
                          <attribute 1> = '" + str(<attribute 1>) + "' AND \
                          <attribute 2> = '" + <attribute 2> + "' AND \
                          <table index> = '<index val>';"
        selectQuery = SimpleStatement(selectQueryVal)
        batch.add(selectQuery);
    # End for loop    
    if debug_flag == 1:
        print('Deleting ' + \
              str(len(argVal1Df)) + \
              ' row(s) from Cassandra DB.'
              );
    cassandraSession.execute(batch)
    if debug_flag == 1:
        print('Deleted ' + \
              str(len(argVal1Df)) + \
              ' row(s) from Cassandra DB.'
              );
    if debug_flag == 1:
        print('### End: deleteRowsFromCassandra ###');

def bookKeep1(argVals):
    debug_flag, session = argVals
    if debug_flag == 1:
        print('### Start: bookKeep1 ###');
    statusCodeList = []
    cTblQuery = 'SELECT DISTINCT A1.attr1 AS "ATTR 1", \
                 A1.attr2 AS "ATTR 2", \
                 B1.attr3 AS "ATTR 3" FROM \
                 DB_NAME.TABLE1_NAME A1 \
                 INNER JOIN \
                 DB_NAME.TABLE2_NAME B1 \
                 ON A1.PK = B1.PK \
                 WHERE (\
                    CAST(REGEXP_SUBSTR(A1.attr4, \'(?<=\[)\d+(?=\])\') AS INT) IN (val1, val2) AND \
                    deleted_from_Cassandra = \'Yes\' \
                 );'
    cTblDf = pd.read_sql(cTblQuery,
                         session
                         );
    if debug_flag == 1:
        print('Length of incidents from TD: ' + str(len(cTblDf)))
    for indexVal, rowVal in cTblDf.iterrows():
        if debug_flag == 1:
            print('Index : ' + str(indexVal) + ' / ' + str(len(cTblDf) - 1));
        statusCodeDict = OrderedDict()
        dec1Val = rowVal['Decision'] == 'VAL1'
        dec2Val = rowVal['Decision'] == 'VAL2'
        decVal  = dec1Val | dec2Val
        if decVal:
            decisionVal = 'VAL3';
        else:
            decisionVal = 'VAL4';
        getResp = requests.get(RESTOVERHTTP + rowVal['Attr 2'] +'/' + decisionVal)
        statusCodeDict.update({'PK'       : int(indexVal),
                               'ATTR 1'   : int(rowVal['ATTR 1']),
                               'ATTR 2'   : rowVal['ATTR 2'],
                               'ATTR 3'   : decisionVal,
                               'Response Code' : int(getResp.status_code)
                               }
                              )
        if debug_flag == 1:
            print('Response Code : ' + str(getResp.status_code));
        statusCodeList.append(statusCodeDict);
    # End for loop
    statusCodeDf = pd.DataFrame(statusCodeList)
    statusCodeDf.index = range(len(statusCodeDf))
    if debug_flag == 1:
        print('### End: bookKeep1 ###');
    return statusCodeDf;

def bookKeep2(argVals):
    debug_flag, cTblDf, poolNum = argVals
    if debug_flag == 1:
        print('### Start: bookKeep2 ###');
    statusCodeList = []
    for indexVal, rowVal in cTblDf.iterrows():
        if debug_flag == 1:
            print('Index : ' + str(indexVal) + ' / ' + str(len(cTblDf) - 1));
        statusCodeDict = OrderedDict()
        dec1Val = rowVal['Attr1'] == '<Val 1>'
        dec2Val = rowVal['Attr1'] == '<Val 2>'
        dec3Val = rowVal['Attr1'] == '<Val 3>'
        dec4Val = rowVal['Attr1'] == '<Val 4>'
        dec5Val = rowVal['Attr1'] == '<Val 5>'
        decVal  = dec1Val | dec2Val | dec3Val | dec4Val | dec5Val
        if decVal:
            decisionVal = '<Common value>';
        else:
            dec1Val = rowVal['Attr2'] == '<Val 1>'
            dec2Val = rowVal['Attr2'] == '<Val 2>'
            dec3Val = rowVal['Attr2'] == '<Val 3>'
            dec4Val = rowVal['Attr2'] == '<Val 4>'
            dec5Val = rowVal['Attr2'] == '<Val 5>'
            decVal  = dec1Val | dec2Val | dec3Val | dec4Val | dec5Val
            if decVal:
                decisionVal = '<Common value>';
        getResp = requests.get(RESTOVERHTTP + str(rowVal['Attr 2']) +'/' + decisionVal)
        statusCodeDict.update({'PK'       : int(indexVal),
                               'ATTR 1'   : int(rowVal['Attr 1']),
                               'ATTR 2'   : rowVal['ATTR 2'],
                               'ATTR 3'   : decisionVal,
                               'Response Code' : int(getResp.status_code)
                               }
                              )
        if debug_flag == 1:
            print('Response Code : ' + str(getResp.status_code));
        statusCodeList.append(statusCodeDict);
    # End for loop
    statusCodeDf = pd.DataFrame(statusCodeList)
    statusCodeDf.index = range(len(statusCodeDf))
    if debug_flag == 1:
        print('Pool # ' + str(poolNum) + ': Success response count = ' + str(len(statusCodeDf[statusCodeDf['Response Code'] == 200])))
        print('Pool # ' + str(poolNum) + ': Failure response count = ' + str(len(statusCodeDf[statusCodeDf['Response Code'] != 200])));
    if debug_flag == 1:
        print('### End: bookKeep2 ###');

def bookKeep3(argVals):
    debug_flag, argVal1, session = argVals
    if debug_flag == 1:
        print('### Start: bookKeep3 ###');
    cTblQuery = 'SELECT DISTINCT A1.attr1 AS "ATTR 1", \
                 A1.attr2 AS "ATTR 2", \
                 B1.attr3 AS "ATTR 3" FROM \
                 DB_NAME.TABLE1_NAME A1 \
                 INNER JOIN \
                 DB_NAME.TABLE2_NAME B1 \
                 ON A1.PK = B1.PK \
                 WHERE (\
                    CAST(REGEXP_SUBSTR(A1.attr4, \'(?<=\[)\d+(?=\])\') AS INT) NOT IN (val1, val2, val3, etc.) AND \
                    <ATTR 1> = \'<Val 1>\' AND \
                    <ATTR 2> = \'<Val 2>\' AND \
              		  <ATTR 3> = \'<VAL 3>\' AND \
                    <ATTR 4> = \'' + argVal1 + '\' \
                 );'
    cTblDf = pd.read_sql(cTblQuery,
                         session
                         );
    if debug_flag == 1:
        print('Length of incidents: ' + str(len(cTblDf)))
    dfLen    = int(len(cTblDf) / 7)
    dfModLen = len(cTblDf) % 7
    if debug_flag == 1:
        print('Length of split files: ' + str(dfLen));
        print('Length of last file  : ' + str(dfModLen));
    cTbl1Df  = cTblDf[(dfLen * 0): (dfLen * 1)]
    cTbl1Df.index = range(len(cTbl1Df))
    cTbl2Df  = cTblDf[(dfLen * 1): (dfLen * 2)]
    cTbl2Df.index = range(len(cTbl2Df))
    cTbl3Df  = cTblDf[(dfLen * 2): (dfLen * 3)]
    cTbl3Df.index = range(len(cTbl3Df))
    cTbl4Df  = cTblDf[(dfLen * 3): (dfLen * 4)]
    cTbl4Df.index = range(len(cTbl4Df))
    cTbl5Df  = cTblDf[(dfLen * 4): (dfLen * 5)]
    cTbl5Df.index = range(len(cTbl5Df))
    cTbl6Df  = cTblDf[(dfLen * 5): (dfLen * 6)]
    cTbl6Df.index = range(len(cTbl6Df))
    cTbl7Df  = cTblDf[(dfLen * 6): (dfLen * 7)]
    cTbl7Df.index = range(len(cTbl7Df))
    if dfModLen > 0:
        cTbl8Df  = cTblDf[(dfLen * 7): len(cTblDf)];
        cTbl8Df.index = range(len(cTbl8Df))
    else:
        cTbl8Df = pd.DataFrame();
    with Pool(8) as proc1:
        proc1.map(bookKeep2,
                  [(debug_flag, cTbl1Df, 1),
                   (debug_flag, cTbl2Df, 2),
                   (debug_flag, cTbl3Df, 3),
                   (debug_flag, cTbl4Df, 4),
                   (debug_flag, cTbl5Df, 5),
                   (debug_flag, cTbl6Df, 6),
                   (debug_flag, cTbl7Df, 7),
                   (debug_flag, cTbl8Df, 8)
                   ]
                  )
    if debug_flag == 1:
        print('Number of rows: ' + str(len(cTblDf)))
        print('### End: bookKeep3 ###');

def bookKeep4(argVals):
    debug_flag, TABLE_NAME, xlsmDf = argVals
    if debug_flag == 1:
        print('### Start: bookKeep4 ###');
    if debug_flag == 1:
        print('Length of incidents from XLSM: ' + str(len(xlsmDf)))
    attr1List = []
    for indexVal, rowVal in xlsmDf.iterrows():
        attr1 = str(int(xlsmDf['ATTR 1'][indexVal]))
        attr1List.append(attr1);
    attr1ListStr = ','.join("'{0}'".format(w) for w in attr1List)
    if debug_flag == 1:
        print('Connecting to Teradata.');
    udaExec = teradata.UdaExec()
    with udaExec.connect("${dataSourceName}") as teradataSession:
        cursor = teradataSession.cursor()
        teradataDbQuery = "SELECT * \
                           FROM DB_NAME." + TABLE_NAME + " WHERE \
                           <ATTR 1> = '<Va1 1>' AND \
                           CAST(REGEXP_SUBSTR(<ATTR 2>, '(?<=\[)\d+(?=\])') AS INT) NOT IN (Val1, Val2, etc.) AND \
                           CAST(<Attr 1> AS BIGINT) IN (" + \
                           attr1ListStr + ");"
        if debug_flag == 1:
            print('Gathering offer IDs from Teradata.');
            print(teradataDbQuery);
        tdDf = pd.read_sql(teradataDbQuery,
                           teradataSession
                           );
        xlsmDf = xlsmDf.merge(tdDf,
                              how = 'outer',
                              on = '<ATTR 1>'
                              )
        if debug_flag == 1:
            print('Number of offers to be reconciled: ' + str(len(xlsmDf)));
    # Closing Teradata connection.
    if debug_flag == 1:
        print('Closing the Teradata connection.');
    cursor.close();
    # Successfully closed the Teradata connection.
    if debug_flag == 1:
        print('Successfully closed the Teradata connection.');
    dfLen    = int(len(xlsmDf) / 7)
    dfModLen = len(xlsmDf) % 7
    if debug_flag == 1:
        print('Length of split files: ' + str(dfLen));
        print('Length of last file  : ' + str(dfModLen));
    xlsm1Df  = xlsmDf[(dfLen * 0): (dfLen * 1)]
    xlsm1Df.index = range(len(xlsm1Df))
    xlsm2Df  = xlsmDf[(dfLen * 1): (dfLen * 2)]
    xlsm2Df.index = range(len(xlsm2Df))
    xlsm3Df  = xlsmDf[(dfLen * 2): (dfLen * 3)]
    xlsm3Df.index = range(len(xlsm3Df))
    xlsm4Df  = xlsmDf[(dfLen * 3): (dfLen * 4)]
    xlsm4Df.index = range(len(xlsm4Df))
    xlsm5Df  = xlsmDf[(dfLen * 4): (dfLen * 5)]
    xlsm5Df.index = range(len(xlsm5Df))
    xlsm6Df  = xlsmDf[(dfLen * 5): (dfLen * 6)]
    xlsm6Df.index = range(len(xlsm6Df))
    xlsm7Df  = xlsmDf[(dfLen * 6): (dfLen * 7)]
    xlsm7Df.index = range(len(xlsm7Df))
    if dfModLen > 0:
        xlsm8Df  = xlsmDf[(dfLen * 7): len(xlsmDf)];
        xlsm8Df.index = range(len(xlsm8Df))
    else:
        xlsm8Df = pd.DataFrame();
    with Pool(8) as proc1:
        proc1.map(bookKeep2,
                  [(debug_flag, xlsm1Df, 1),
                   (debug_flag, xlsm2Df, 2),
                   (debug_flag, xlsm3Df, 3),
                   (debug_flag, xlsm4Df, 4),
                   (debug_flag, xlsm5Df, 5),
                   (debug_flag, xlsm6Df, 6),
                   (debug_flag, xlsm7Df, 7),
                   (debug_flag, xlsm8Df, 8)
                   ]
                  )
    if debug_flag == 1:
        print('Number of rows: ' + str(len(xlsmDf)))
        print('### End: bookKeep4 ###');

def createPayloadDfFromCdbDf(debug_flag, origDf):
    if debug_flag == 1:
        print('### Start: createPayloadDfFromCdbDf function ###');
    resultDf  = pd.DataFrame()
    explosionIndex = 0
    temp1Df   = origDf.drop(['<Attr 1>'],
                            axis = 1
                            )
    temp1Df.sort_index(axis = 1,
                       inplace = True
                       )
    temp1Cols = temp1Df.columns.tolist()
    if debug_flag == 1:
        print('Length of Df = ' + str(len(origDf)));
    for index, rows in origDf.iterrows():
        requestDf = pd.io.json.json_normalize(json.loads(origDf['<Attr 1>'][index]))
        requestDf.sort_index(axis = 1,
                             inplace = True
                             )
        temp2Df = requestDf.drop(['<Attr 2>',
                                  '<Attr 3>',
                                  '<Attr 4>',
                                  '<Attr 5>',
                                  '<Attr 6>',
                                  '<Attr 7>',
                                  ],
                                 axis = 1
                                 )
        temp2Df.index = [explosionIndex]
        temp2Df.sort_index(axis = 1,
                           inplace = True
                           )
        temp2Cols = temp2Df.columns.tolist()
        attr2Df = pd.io.json.json_normalize(requestDf['Attr 2'][0].pop())
        for colName in ['col  1',
                        'col  2',
                        'col  3',
                        'col  4',
                        'col  5',
                        'col  6',
                        'col  7',
                        'col  8',
                        'col  9',
                        'col 10',
                        'col 11',
                        'col 12'
                        ]:
            if colName not in attr2Df.columns:
                attr2Df[colName] = '';
        attr2Df.index = [explosionIndex]
        attr2Df.sort_index(axis = 1,
                           inplace = True
                           )
        attr2Cols  = attr2Df.columns.tolist()
        attr3List = requestDf['<Attr 3>']
        attr3Cols = pd.io.json.json_normalize(attr3List.item()[0]).columns.tolist()
        attr4List  = requestDf['<Attr 4>']
        attr4Cols  = pd.io.json.json_normalize(attr4List.item()[0]).columns.tolist()
        if len(attr3List.item()) > 1:
            for pIndexVal in range(len(attr3List.item())):
                doubleCountFlag = 1;
                if debug_flag == 1:
                    print('pIndexVal = ' + str(pIndexVal));
                t1Df = temp1Df[index : index + 1]
                t1Df.index = [explosionIndex]
                t1Df.sort_index(axis = 1,
                                inplace = True
                                )
                t2Df = temp2Df
                t2Df.index = [explosionIndex]
                t2Df.sort_index(axis = 1,
                                inplace = True
                                )
                t3Df = itemsDf
                t3Df.index = [explosionIndex]
                t3Df.sort_index(axis = 1,
                                inplace = True
                                )
                attr3Df = pd.io.json.json_normalize(attr3List.item()[pIndexVal])
                attr3Df.index = [explosionIndex]
                attr3Df.sort_index(axis = 1,
                                   inplace = True
                                   )
                temp3Df = pd.concat([t1Df,
                                     t2Df,
                                     t3Df,
                                     attr3Df
                                     ],
                                    axis = 1
                                    )
                temp3Df.index = [explosionIndex]
                temp3Df.columns = temp1Cols + \
                                  temp2Cols + \
                                  attr2Cols + \
                                  attr3Cols
                temp3Df.sort_index(axis = 1,
                                   inplace = True
                                   )
                if len(attr4List.item()) > 1:
                    for oIndexVal in range(len(attr4List.item())):
                        if debug_flag == 1:
                            print('oIndexVal = ' + str(oIndexVal) + ', pIndexVal = ' + str(pIndexVal));
                        t3Df = temp3Df
                        t3Df.index = [explosionIndex]
                        t3Df.sort_index(axis = 1,
                                        inplace = True
                                        )
                        attr4Df = pd.io.json.json_normalize(attr4List.item()[oIndexVal])
                        attr4Df.index = [explosionIndex]
                        attr4Df.sort_index(axis = 1,
                                           inplace = True
                                           )
                        if debug_flag == 1:
                            print('<Attr 3> and <Attr 4> based.')
                            print('explosionIndex = ' + str(explosionIndex));
                        if (index > 0):
                            r1Df = resultDf
                            r1Df.index = range(explosionIndex)
                            temp4Df = pd.concat([t3Df,
                                                 attr4Df
                                                 ],
                                                axis = 1
                                                )
                            temp4Df.index = [explosionIndex]
                            temp4Df.sort_index(axis = 1,
                                               inplace = True
                                               )
                            resultDf = pd.concat([temp4Df,
                                                  r1Df
                                                  ],
                                                 axis = 0
                                                 );
                        else:
                            temp4Df = pd.concat([temp3Df,
                                                 attr4Df
                                                 ],
                                                axis = 1
                                                )
                            temp4Df.index = [explosionIndex]
                            temp4Df.sort_index(axis = 1,
                                               inplace = True
                                               )
                            resultDf = pd.concat([temp4Df,
                                                  resultDf
                                                  ],
                                                 axis = 0
                                                 );
                        if debug_flag == 1:
                            print('Counting explosionIndex : 1, oIndexVal = ' + str(oIndexVal));
                        explosionIndex = explosionIndex + 1;
                    doubleCountFlag = 0;
                else:
                    attr4Df = pd.io.json.json_normalize(attr4List.item()[0])
                    attr4Df.index = [explosionIndex]
                    attr4Df.sort_index(axis = 1,
                                       inplace = True
                                       )
                    if debug_flag == 1:
                        print('<Attr 3> based.')
                        print('explosionIndex = ' + str(explosionIndex));
                    if (index > 0):
                        r1Df = resultDf
                        r1Df.index = range(explosionIndex)
                        temp4Df = pd.concat([temp3Df,
                                             attr4Df
                                             ],
                                            axis = 1
                                            )
                        temp4Df.index = [explosionIndex]
                        temp4Df.sort_index(axis = 1,
                                           inplace = True
                                           )
                        resultDf = pd.concat([temp4Df,
                                              r1Df
                                              ],
                                             axis = 0
                                             );
                    else:
                        temp4Df = pd.concat([temp3Df,
                                             attr4Df
                                             ],
                                            axis = 1
                                            )
                        temp4Df.index = [explosionIndex]
                        resultDf = pd.concat([temp4Df,
                                              resultDf
                                              ],
                                             axis = 0
                                             );
                if debug_flag == 1:
                    print('Counting explosionIndex : 2, pIndexVal = ' + str(pIndexVal));
                explosionIndex = explosionIndex + doubleCountFlag;
                doubleCountFlag = 1;
        else:
            attr3Df = pd.io.json.json_normalize(attr3List.item()[0])
            attr3Df.index = [explosionIndex]
            attr3Df.sort_index(axis = 1,
                                inplace = True
                                )
            attr3Cols = attr3Df.columns.tolist();
            if len(attr4List.item()) > 1:
                for oIndexVal in range(len(attr4List.item())):
                    if debug_flag == 1:
                        print('oIndexVal = ' + str(oIndexVal));
                    t1Df = temp1Df[index : index + 1]
                    t1Df.index = [explosionIndex]
                    t1Df.sort_index(axis = 1,
                                    inplace = True
                                    )
                    t2Df = temp2Df
                    t2Df.index = [explosionIndex]
                    t2Df.sort_index(axis = 1,
                                    inplace = True
                                    )
                    t3Df = attr2Df
                    t3Df.index = [explosionIndex]
                    t3Df.sort_index(axis = 1,
                                    inplace = True
                                    )
                    t4Df = attr3Df
                    t4Df.index = [explosionIndex]
                    t4Df.sort_index(axis = 1,
                                    inplace = True
                                    )
                    attr4Df = pd.io.json.json_normalize(attr4List.item()[oIndexVal])
                    attr4Df.index = [explosionIndex]
                    attr4Df.sort_index(axis = 1,
                                       inplace = True
                                       )
                    temp3Df = pd.concat([t1Df,
                                         t2Df,
                                         t3Df,
                                         t4Df,
                                         attr4Df
                                         ],
                                        axis = 1
                                        )
                    temp3Df.index = [explosionIndex]
                    temp3Df.columns = temp1Cols + \
                                      temp2Cols + \
                                      attr1Cols + \
                                      attr2Cols + \
                                      attr3Cols
                    temp3Df.sort_index(axis = 1,
                                       inplace = True
                                       )
                    if debug_flag == 1:
                        print('<Attr 4> based.')
                        print('explosionIndex = ' + str(explosionIndex));
                    if (index > 0):
                        r1Df = resultDf
                        r1Df.index = range(explosionIndex)
                        resultDf = pd.concat([temp3Df,
                                              r1Df
                                              ],
                                             axis = 0
                                             );
                    else:
                        resultDf = pd.concat([temp3Df,
                                              resultDf
                                              ],
                                             axis = 0
                                             );
                    if debug_flag == 1:
                        print('Counting explosionIndex : 3, oIndexVal = ' + str(oIndexVal));
                    explosionIndex = explosionIndex + 1;
            else:
                t1Df = temp1Df[index : index + 1]
                t1Df.index = [explosionIndex]
                t1Df.sort_index(axis = 1,
                                inplace = True
                                )
                t2Df = temp2Df
                t2Df.index = [explosionIndex]
                t2Df.sort_index(axis = 1,
                                inplace = True
                                )
                t3Df = itemsDf
                t3Df.index = [explosionIndex]
                t3Df.sort_index(axis = 1,
                                inplace = True
                                )
                attr4Df       = pd.io.json.json_normalize(attr4Lost[0])
                attr4Df.index = [explosionIndex]
                attr4Df.sort_index(axis = 1,
                                   inplace = True
                                   )
                temp3Df = pd.concat([t1Df,
                                     t2Df,
                                     t3Df,
                                     attr3Df,
                                     attr4Df
                                     ],
                                    axis = 1
                                    )
                temp3Df.columns = temp1Cols + \
                                  temp2Cols + \
                                  attr2Cols + \
                                  attr3Cols + \
                                  attr4Cols
                temp3Df.sort_index(axis = 1,
                                   inplace = True
                                   )
                if debug_flag == 1:
                    print('Normal.')
                    print('explosionIndex = ' + str(explosionIndex));
                if (index > 0):
                    r1Df = resultDf
                    r1Df.index = range(explosionIndex)
                    resultDf = pd.concat([temp3Df,
                                          r1Df
                                          ],
                                         axis = 0
                                         );
                else:
                    resultDf = pd.concat([temp3Df,
                                          resultDf
                                          ],
                                         axis = 0
                                         );
                if debug_flag == 1:
                    print('Counting explosionIndex : 4');
                explosionIndex = explosionIndex + 1;
        if debug_flag == 1:
            print('Index = ' + str(index));
    if debug_flag == 1:
        print('### End: createPayloadDfFromCdbDf function ###');
    return resultDf;

def createPayloadD(argVals):
    debug_flag, queryResultDf = argVals
    itemDf = createPayloadDfFromCdbDf(debug_flag, queryResultDf)
    return itemDf;

def insertNulls(listItems):
    resultList = []
    for listElement in listItems:
        resultList.append('')
        resultList.append(listElement)
    return resultList;

# Stitching together the data files.
def stitchDataFiles(debug_flag, filePath, incidentsFile):
    stitchedDf = pd.DataFrame()
    if debug_flag == 1:
        print('### Start: stitchDataFiles function ###')
        print('Gathering the individual files and stitching them together.')
    fileNames  = glob.glob(os.path.join(filePath, "*.txt"))
    indivDf    = (pd.read_csv(eachFile,
                              header = 0,
                              sep = '\t'
                              ) for eachFile in fileNames
                  )
    stitchedDf = pd.concat(indivDf,
                           ignore_index = True
                           )
    stitchedDf.index = range(len(stitchedDf))
    if debug_flag == 1:
        print('Length of stitched data frame: ' + str(len(stitchedDf)))
        print('Writing to the specified file: ' + incidentsFile);
    stitchedDf.to_csv(incidentsFile,
                      sep = '\t',
                      index = False
                      )
    if debug_flag == 1:
        print('### End: stitchDataFiles function ###');

def copyFilesFromUdriveToMbp(debug_flag):
    if debug_flag == 1:
        print('### Start: copyFilesFromUdriveToMbp function ###')
        print('Copying the individual incident files and stitching them together.')
    shutil.copy2(currMuFilePath,
                 muDirPath
                 )
    shutil.copy2(currAbFilePath,
                 abDirPath
                 )
    shutil.copy2(currPeFilePath,
                 PFPATH
                 )
    shutil.copy2(currPe107373FilePath,
                 PFPATH
                 )
    stitchDataFiles(debug_flag,
                    muDirPath,
                    incidentsFile
                    )
    if debug_flag == 1:
        print('### End: copyFilesFromUdriveToMbp function ###');

def filterFunc(debug_flag, incidentsFile):
    if debug_flag == 1:
        print('### Start: filterFunc function ###')
        print('Reading the specified file.');
    monthlyDf = pd.read_csv(incidentsFile,
                            header = 0,
                            sep = '\t',
                            index_col = False)
    if debug_flag == 1:
        print('Length before filtering: ' + str(len(monthlyDf)));
    monthlyDf = monthlyDf[((((monthlyDf['<ATTR 1>'] != '<Val 1>') &
                             (monthlyDf['<ATTR 1>'] != '<Val 2>') &
                             (monthlyDf['<ATTR 1>'] != '<Val 3>') &
                             (monthlyDf['<ATTR 1>'] != '<Val 4>')
                             ) |
                            (pd.to_datetime(monthlyDf['<ATTR 2>']) < '2012-12-30') |
                            (pd.to_datetime(monthlyDf['<ATTR 2>']) > '2013-09-01')
                            ) &
                           (monthlyDf['<ATTR 1>'] != '<Val 5>') &
                           (monthlyDf['<ATTR 1>'] != '<Val 6>')
                           )
                          ]
    if debug_flag == 1:
        print('Length after filtering: ' + str(len(monthlyDf)))
        print('Writing to the specified file.');
    monthlyDf.to_csv(incidentsFile,
                     sep = '\t',
                     index = False
                     )
    if debug_flag == 1:
        print('### End: filterFunc function: ###');

## Renaming columns.
#if debug_flag == 1:
#    print('Renaming columns.');
#dailyDf.rename(columns = {'<Col 1>':'<col 1>',
#                          '<Col 2>':'<col 2>',
#                          '<Col 3>':'<col 3>',
#                          '<Col 4>':'<col 4>',
#                          '<Col 5>':'<col 5>',
#                          '<Col 6>':'<col 6>',
#                          },
#               inplace = True
#               )

def weekDescription(weekDate):
    """
    Return a description of the calendar week (Sunday to Saturday)
    containing the date d, avoiding repetition.
    >>> weekDescription(date(2013, 12, 30))
    'Dec 29 - Jan 4'
    >>> weekDescription(date(2014, 1, 25))
    'Jan 19 - Jan 25'
    >>> weekDescription(date(2014, 1, 26))
    'Jan 26 - Feb 1'
    """
    begin = weekDate - timedelta(days = weekDate.isoweekday() % 7)
    end = begin + timedelta(days = 6)

    assert begin.isoweekday() == 7 # Sunday
    assert end.isoweekday() == 6   # Saturday
    assert begin <= weekDate <= end

#    if begin.year != end.year:
#        fmt = '{0:%b} {0.day}, {0.year} - {1:%b} {1.day}, {1.year}'
#    elif begin.month != end.month:
#        fmt = '{0:%b} {0.day} - {1:%b} {1.day}, {1.year}'
#    else:
#        fmt = '{0:%b} {0.day} - {1.day}, {1.year}'
#    if begin.month != end.month:
#        fmt = '{0:%b} {0.day} - {1:%b} {1.day}'
#    else:
#        fmt = '{0:%b} {0.day} - {{1.day}'
    fmt = '{0:%b} {0:%d} - {1:%b} {1:%d}'
    return fmt.format(begin, end)

# Get the month of the earliest incident.
def getEarliestCreatedMonth(reviewFileDf):
    sortedDf = reviewFileDf.sort_values(['<Date Attr>'])
    earliestDate = pd.to_datetime(sortedDf[:1]['<Date Attr>'],
                                  infer_datetime_format = True
                                  )
    return int(earliestDate.dt.month);

convToStr = lambda attr1Val: '\'' + str(attr1Val) + '\','

statusAttr = lambda subjectVal: re.search('(?<=\[)[A-Z|a-z| |-]+(?=\])', subjectVal).group(0)
attr1Val   = lambda subjectVal: int(re.search('(^-|\d)+', str(subjectVal)).group(0))
attr2Val   = lambda subjectVal: re.sub('"|','', subjectVal)

remove_strings = lambda incId : not re.match('[A-Z|a-z]*', incId).group(0)
replaceQuotes = lambda stringVal: re.sub('\'|"|’|–|“|”','\'\'', str(stringVal))
replaceElongatedHyphen = lambda stringVal: re.sub('–','-', str(stringVal))
replaceDollarSign = lambda stringVal: re.sub('\$','\$$', str(stringVal))
parseAttr1 = lambda attr1Val: re.search('(- )(P\d)', attr1Val).group(2)
attr4Pattern = lambda attr4Val: re.split(' - ', attr4Val)[1]

def removeDups(debug_flag, oldDataFile, newDataFile, dataDf):
    if debug_flag == 1:
        print('### Start: removeDups function ###')
    oldDataDf = pd.read_csv(oldDataFile,
                            header = 0,
                            sep = '\t',
                            index_col = False
                            )
    newDataDf = pd.read_csv(newDataFile,
                            header = 0,
                            sep = '\t',
                            index_col = False
                            )
    oldDataDf.drop_duplicates(subset = ['<Attr 1>', '<Attr 2>'],
                              keep = 'first',
                              inplace = True
                              )
    newDataDf.drop_duplicates(subset = ['<Attr 2>', '<Attr 2>'],
                              keep = 'first',
                              inplace = True
                              )
    odDf = oldDataDf[['<Attr 1>',
                      '<Attr 2>',
                      '<Attr 3>'
                      ]].set_index(['<Attr 1>', '<Attr 2>'])
    ndDf = newDataDf[['<Attr 1>',
                      '<Attr 2>',
                      '<Attr 3>'
                      ]].set_index(['<Attr 1>', '<Attr 2>'])
    odDf.update(ndDf)
    dataDf['<Attr 4>'] = odDf.values
    odDf = oldDataDf[['<Attr 1>',
                      '<Attr 1>',
                      '<Attr 5>'
                      ]].set_index(['<Attr 1>', '<Attr 2>'])
    ndDf = newDataDf[['<Attr 1>',
                      '<Attr 1>',
                      '<Attr 5>'
                      ]].set_index([<Attr 1>', '<Attr 2>'])
    odDf.update(ndDf)
    dataDf['<Attr 5>'] = odDf.values
    odDf = oldDataDf[['<Attr 1>',
                      '<Attr 1>',
                      '<Attr 6>'
                      ]].set_index(['<Attr 1>', '<Attr 2>'])
    ndDf = newDataDf[['<Attr 1>',
                      '<Attr 1>',
                      '<Attr 6>'
                      ]].set_index([<Attr 1>', '<Attr 2>'])
    odDf.update(ndDf)
    dataDf['<Attr 6>'] = odDf.values
    if debug_flag == 1:
        print('### End: removeDups function ###');
    return dataDf;

# Changing timestamp from US/Central to US/Pacific (The timestamp is removed):
# For example:
#        CENTRAL      -->         PDT         -->      'Date Created'
# 05/12/2016 12:01 AM --> 05/11/2016 10:01 PM -->   05/11/2016 22:01:00
# 05/12/2016 01:01 AM --> 05/11/2016 11:01 PM -->   05/11/2016 23:01:00
# 05/12/2016 01:59 AM --> 05/12/2016 11:59 PM -->   05/11/2016 23:59:00
# 05/12/2016 02:00 AM --> 05/11/2016 12:00 AM -->   05/12/2016 00:00:00
# 05/12/2016 02:01 AM --> 05/12/2016 12:01 AM -->   05/12/2016 00:01:00
pstDate = lambda dateCreated: datetime.strftime(pd.to_datetime(dateCreated).tz_localize('US/Central',
                                                                                        ambiguous = False,
                                                                                        ).astimezone(timezone('US/Pacific')),
                                                '%Y-%m-%d %H:%M:%S'
                                                )

# Creating a column to display week start and end dates.
weekDateUpdate = lambda weekDate: weekDescription(weekDate)
#weekDateUpdate = lambda weekDate: weekDescription(datetime.strptime(weekDate, '%Y-%m-%d %H:%M:%S'))

# Functions used in creating a column to display week number:
# Fiscal year starts from first week of February.
monthNumVal   = lambda dateVal: int(dateVal.strftime('%m'))
yearNumVal    = lambda dateVal: int(dateVal.strftime('%Y'))
weekNumVal    = lambda dateVal: int(dateVal.strftime('%U'))
weekMinusVal  = lambda dateVal: 5 if weekNumVal(date(int(dateVal.strftime('%Y')), 2, 1)) == 6 else 4
weekPlusVal   = lambda dateVal: 47 if weekNumVal(date(int(dateVal.strftime('%Y')), 2, 1)) == 6 else 48
weekNumUpdate = lambda dateVal: weekNumVal(dateVal) - weekMinusVal(date(yearNumVal(dateVal), 2, 1)) if monthNumVal(dateVal) > 1 else weekNumVal(dateVal) + weekPlusVal(date(yearNumVal(dateVal), 2, 1))

convStrToDate = lambda dateVal: datetime.strptime(dateVal, '%m/%d/%Y %H:%M %p').date()

# Find the max length of the index.
def getIndexWidth(resultDf):
    return max([len(str(y)) for y in resultDf.index.values]);

# Find the max of the length of each column and its value.
# Defaults to a minmum column width of 10.
def getColumnWidths(resultDf):
    return [max([10] + [len(str(y)) for y in resultDf[columnNames].values] + [len(columnNames)]) for columnNames in resultDf.columns]

# Find the difference between two dates.
def daysDiff(resultDf):
    numDays = resultDf['<Attr 1>'] - resultDf['<Attr 2>']
    return numDays.days;

# Creating a column to display the month.
monthNumUpdate = lambda monthDate: datetime.strftime(pd.to_datetime(monthDate), '%b')

# Convert date strings to date format.
convertToDateTime = lambda dateToBeConverted: datetime.strftime(pd.to_datetime(dateToBeConverted),
                                                                '%Y-%m-%d %H:%M:%S'
                                                                )

# prevFileDf = pd.read_csv(prevFilePath,
#                          header = 0,
#                          index_col = False,
#                          encoding = "ISO-8859-1"
#                          )

# for index, rowVals in dataFileDf.iterrows():
#    replacedRowVals = rowVals[:-1].str.replace("\'",
#                                               ""
#                                               )
#    replacedRowVals = replacedRowVals.str.replace("\"",
#                                                  ""
#                                                  )
#    replacedRowVals = replacedRowVals.str.replace("$",
#                                                  "$$"
#                                                  )
#    formattedRowVals = ', '.join('"{0}"'.format(word) \
#                       for word in replacedRowVals.to_csv(sep = '\t',
#                                                          header = None,
#                                                          index = False
                                                           ).split('\n')
                                                          )

parseDateValOps    = lambda dateString: dateString[-14:][:10]
parseDateValHld    = lambda dateString: dateString[-17:][:10]
countLines         = lambda fileString: open(fileString, 'r', encoding = 'utf-8').read().count('\n') + 1
convDateTimeToDate = lambda dateString: datetime.strftime(pd.to_datetime(dateString).date(), '%m-%d-%Y')

def numDaysDiff(dateValDf):
    return (pd.to_datetime(dateValDf['<Attr 1>']) - pd.to_datetime(dateValDf['<Attr 2>'])).days;

addWkText = lambda fyWkNum: 'Wk ' + str(fyWkNum);

def autoLabelBarCharts(ax, rects):
    """
    Attach a text label above each bar displaying its height
    """
    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + (rect.get_width() / 2.0),
                1.05 * height,
                '%d' % int(height),
                ha = 'center',
                va = 'bottom'
                );

# Creating bins.
def getBins(rowVal):
    incidentDateCreated = pd.to_datetime(rowVal['<Attr 1>'])
    holdVal             = rowVal['<Attr 2>']
    if holdVal == 'Hold':
        if abs((pd.to_datetime(datetime.strptime(todayFormattedDate, '%m-%d-%Y').date()) - incidentDateCreated).days) < 7:
            binVal = '< 7 days'
        elif abs((pd.to_datetime(datetime.strptime(todayFormattedDate, '%m-%d-%Y').date()) - incidentDateCreated).days) < 14:
            binVal = '7 - 14 days'
        else:
            binVal = '> 14 days'
    else:
        binVal = 'NA';
    return binVal;

# Create a chart for age of Attr1 based on Attr Val1.
def getAgeChart(debug_flag, yearNum, valDf, chartFileName):
    if debug_flag == 1:
        print('### Start: getAgeChart function ###');

    attrValDf = valDf[valDf['<Attr 1>'] >= date(yearNum, 2, 1)]
    attrValDf = attrValDf[attrValDf['<Attr 1>'] < date(yearNum + 1, 2, 1)]
    
    attrValDf = attrValDf[['<Attr 1>', '<Attr 2>']]
    attrValDf['<Attr Bin>'] = attrValDf.apply(getBins,
                                              axis = 1
                                              )
    attrValDf['<Bin Val1>'] = (attrValDf['<Attr Bin>'] == '<Bin Val1>') * 1
    attrValDf['<Bin Val2>'] = (attrValDf['<Attr Bin>'] == '<Bin Val2>') * 1
    attrValDf['<Bin Val3>'] = (attrValDf['<Attr Bin>'] == '<Bin Val3>') * 1
    
    attrBinDf = attrValDf[['<Attr Bin>',
                           '<Bin Val1>',
                           '<Bin Val2>',
                           '<Bin Val3>'
                           ]
                          ].pivot_table(values = ['<Bin Val1>',
                                                  '<Bin Val2>',
                                                  '<Bin Val3>'
                                                  ],
                            index = ['<attr Bin>'],
                            fill_value = 0,
                            aggfunc = np.sum
                            )
    
    chartDf = pd.DataFrame({'Age' : [attrBinDf.ix[0].sum(),
                                     attrBinDf.ix[1].sum(),
                                     attrBinDf.ix[2].sum()
                                     ]
                            }
                           )
    xTicks  = list(range(len(chartDf)))
    # xLabels = list(chartDf.index.values)
    xLabels = ['<Bin Val1>', '<Bin Van2>', '<Bin Val3>']

    fig = plt.figure()
    ax = fig.add_subplot(111)
    barColors = ['green',
                 'orange',
                 'red'
                 ]
    ax.bar(list(range(len(chartDf))),
           chartDf['Age'].tolist(),
           color = barColors,
           label = None,
           width = 0.25
           )
    ax.set_facecolor('w')
    ax.spines['bottom'].set_color('k')
    ax.spines['left'].set_color('k')
    ax.set_xticks(xTicks)
    ax.set_xticklabels(xLabels,
                       rotation = 0
                       )
    plt.title('Age of Attr (' + str(chartDf['Age'].sum()) + ' On <Attr>)')
    
    plt.xlabel('')
    plt.ylabel('')
    
    plt.grid(False)
    plt.tight_layout()
    
    plt.savefig(chartFileName,
                dpi = 1200,
                bbox_inches = 'tight',
                pad_inches = 0.25,
                facecolor = 'w',
                transparent = True
                )
    if debug_flag == 1:
        print('### End: getAgeChart function ###');

def getEarliestIncidentDate(debug_flag):
    path1FileNames = glob.glob(os.path.join(<PATH1>, "*/*."))
    path2FileNames = glob.glob(os.path.join(<PATH2>, "*/*.txt"))
    if debug_flag == 1:
        print('Reading in the individual files.')
    indivDf = (pd.read_csv(eachFile,
                           header = 0,
                           sep = '\t'
                           ) for eachFile in [path1FileNames,
                                              path2FileNames
                                              ]
               )
    if debug_flag == 1:
        print('Stitching the individual files.')
    stitchedDf = pd.concat(indivDf,
                           ignore_index = True
                           )
    earliestDate = pd.to_datetime(stitchedDf['<Date Attr>'].sort_values(['<Date Attr>'])[0])
    if debug_flag == 1:
        print('Earliest "<Date Attr>" is ' + earliestDate + '.');
    return earliestDate;

recvMailFormat = lambda emailAddr: "{} ".format(emailAddr.split(' <')[0],
                                                emailAddr
                                                )

def sendReportViaEmail(debug_flag, messageFile):
    if debug_flag == 1:
        print('### Start: sendReportViaEmail function ###');
    emailMessage  = ''
    with open(messageFile, 'r') as msgFile:
        emailMessage  = msgFile.read()
    fromEmailAddr = ''
    with open(messageFile, 'r') as msgFile:
        fromEmailAddr = msgFile.readlines()[0].split('From: ')[1]
    emailAddrList = []
    with open(messageFile, 'r') as msgFile:
        allToEmails = msgFile.readlines()[1].split('To: ')[1]
        allToEmails = allToEmails.split(sep = ', ');
        for eachAddr in allToEmails:
            toEmails = eachAddr.split(sep = '<')[1]
            toEmails = toEmails.split(sep = '>')[0]
            emailAddrList.append(toEmails);
    with open(messageFile, 'r') as msgFile:
        allCcEmails = msgFile.readlines()[2].split('Cc: ')[1]
        allCcEmails = allCcEmails.split(sep = ', ')
        for eachAddr in allCcEmails:
            ccEmails = eachAddr.split(sep = '<')[1]
            ccEmails = ccEmails.split(sep = '>')[0]
            emailAddrList.append(ccEmails);
 
    # Send the message via local SMTP server.
    session = smtplib.SMTP('<local smtp server>')
    # sendmail function takes 3 arguments:
    # (1) Sender's e-mail address
    # (2) Recipient's e-mail address
    # (3) Message to send (string).
    if debug_flag == 1:
        print('Sending mails to ' + str(emailAddrList));
    session.sendmail(fromEmailAddr,
                     emailAddrList,
                     emailMessage
                     )
    # session.sendmail(fromEmailAddr,
    #                  ', '.join(toEmailAddr) + ', ' + ', '.join(ccEmailAddr),
    #                  emailMessage
    #                  )
    print ("Successfully sent email")
    session.quit()
    if debug_flag == 1:
        print('### End: sendReportViaEmail function ###');

def stylizeBaseDataWorksheet(debug_flag, dfName, sheetName, xlsxFile):
    if debug_flag == 1:
        print('### Start: stylizeBaseDataWorksheet function ###');

    # Variables related to Excel formatting
    headerFormat = {'bold'       : True,
                    'border'     : 1,
                    'font_color' : 'white',
                    'bg_color'   : '#1F4E79'
                    }
    indexCellFormat = {'align'     : 'left',
                       'valign'    : 'vcenter',
                       'font_size' : 12
                       }
    yearMonthHeaderFormat = {'align'     : 'center',
                             'valign'    : 'vcenter',
                             'font_size' : 12
                             }
    allBorderFormat = {'top'    : 1,
                       'right'  : 1,
                       'bottom' : 1,
                       'left'   : 1
                       }
    dataCellFormat = {'valign'    : 'vcenter',
                      'font_size' : 12
                      }
    fyCellFmt = {'valign'    : 'vcenter',
                 'font_size' : 12,
                 'bold'      : True,
                 'bg_color'  : '#FFF2CC'
                 }

    # Get the xlsxwriter objects from the data frame writer object.
    workBook  = xlsxFile.book
    workSheet = xlsxFile.sheets[sheetName]
    formatWb  = workBook.add_format

    # Get the number of rows and columns.
    numRows = len(dfName)
    numCols = len(dfName.columns)
    lastCol = numCols + 2

    # All rows and columns are 0-indexed in the data frame, but in excel columns are 1-indexed.
    # The index starts from row 4, column 3.
    indexCells  = 'C4:C' + str(numRows + 3)
    # The data starts from row 4, column D, excluding the last 6 columns.
    dataCells   = 'D4:' + chr(lastCol + ord('A')) + str(numRows + 3 - 6)
    # The header starts from row 3, column 4.
    headerRow = 'D3:' + chr(lastCol + ord('A')) + '3'
    # FY cells start from row 4, column P and the last 6 columns.
    fyCells   = 'P4:' + chr(lastCol + ord('A')) + str(numRows + 3)

    workSheet.set_column(indexCells,
                         None,
                         formatWb(indexCellFormat)
                         )

    workSheet.set_column(headerRow,
                         None,
                         formatWb(yearMonthHeaderFormat)
                         )

    # Change cell formats.
    ## Default font size is assumed to be 11.
    ## Default font type is assumed to be Calibri.
    ## Default row height is assumed to be 15.
    commonFormat  = formatWb(dataCellFormat)
    cellBorderFormat = formatWb(allBorderFormat)
    fyCellFormat = formatWb(fyCellFmt)

    # Set filler column widths.
    workSheet.set_column('A:B', 1.5)
    workSheet.set_column(chr(lastCol + 1 + ord('A')) + ':' + chr(lastCol + 1 + ord('A')), 1.5)
    # workSheet.column_dimensions['A'].width = 3
    # workSheet.column_dimensions['B'].width = 3
    # workSheet.column_dimensions[chr(lastCol + 1 + ord('A'))].width = 3

    # In Excel, a conditional format is superimposed over the existing cell format.
    # Not all cell format properties can be modified.
    # Properties that cannot be modified in a conditional format are font name,
    # font size, superscript and subscript, diagonal borders, all alignment
    # properties and all protection properties.

    if debug_flag == 1:
        print('Setting index col format.');
    workSheet.conditional_format(indexCells,
                                 {'type'  : 'no_errors',
                                  'format': formatWb(headerFormat)
                                  }
                                 )

    # Setting data cells format.
    if debug_flag == 1:
        print('Setting data cells format.');
    workSheet.set_column(dataCells,
                         None,
                         commonFormat
                         )
    if debug_flag == 1:
        print('Draw borders for data cells.');
    workSheet.conditional_format(dataCells,
                                 {'type'  : 'no_errors',
                                  'format': cellBorderFormat
                                  }
                                 )

    # Setting header row format.
    if debug_flag == 1:
        print('Setting header row format.');
    workSheet.conditional_format(headerRow,
                                 {'type'  : 'no_errors',
                                  'format': formatWb(headerFormat)
                                  }
                                 )
    # Setting fiscal year cells format.
    if debug_flag == 1:
        print('Setting fiscal year cells format.');
    workSheet.set_column(fyCells,
                         None,
                         fyCellFormat
                         )
    if debug_flag == 1:
        print('Draw borders for fiscal year cells.');
    workSheet.conditional_format(fyCells,
                                 {'type'  : 'no_errors',
                                  'format': cellBorderFormat
                                  }
                                 )


    # Set index column widths.
    columnNumber = 2
    workSheet.set_column(columnNumber, columnNumber, getIndexWidth(dfName))

    # Set data cell widths.
    # The '+ 1' factor is to account for width differences between MacOS and Windows.
    columnNumber = 3
    for columnWidth in getColumnWidths(dfName):
        workSheet.set_column(columnNumber, columnNumber, columnWidth + 1)
        columnNumber += 1;

    # Closing xlsxWriter and saving Excel file.
    if debug_flag == 1:
        print("Closing xlsxWriter and saving Excel file.");
    xlsxFile.save();
    if debug_flag == 1:
        print('### End: stylizeBaseDataWorksheet function ###');

def stylizeAdditionalWorksheet(debug_flag, dfName, XLSXFILE, xlsxWorkbook, sheetName):
    if debug_flag == 1:
        print('### Start: stylizeAdditionalWorksheet function ###');

    # Get the work-sheet.
    workSheet = xlsxWorkbook[sheetName]

    # Get the number of rows and columns.
    numRows = len(dfName)
    numCols = len(dfName.columns)
    lastCol = numCols + 3

    thin = Side(border_style = 'thin',
                color        = '000000' # black
                )
    cellBorder = Border(top    = thin,
                        right  = thin,
                        bottom = thin,
                        left   = thin
                        )

    # All rows and columns are 0-indexed in the data frame, but in excel columns are 1-indexed.
    # The header starts from row 3, column 4.
    # headerRows = 'D3' through "D + chr(lastCol) + '3'".
    if debug_flag == 1:
        print('Setting header cells format.');
    headerRowAlignment = Alignment(horizontal = 'center',
                                   vertical   = 'center'
                                   )
    headerRowFill = PatternFill(fill_type = 'solid',
                                start_color = '1F4E79',
                                end_color   = '1F4E79'
                                )
    headerRowFont = Font(size = 12,
                         bold = True,
                         color = 'FFFFFF' # white
                         )
    for colLetter in range(ord('D'), lastCol + ord('A')):
        workSheet[chr(colLetter) + '3'].alignment = headerRowAlignment
        workSheet[chr(colLetter) + '3'].border    = cellBorder
        workSheet[chr(colLetter) + '3'].fill      = headerRowFill
        workSheet[chr(colLetter) + '3'].font      = headerRowFont

    if debug_flag == 1:
        print('Setting index column format.');
    # The index starts from row 4, column 3.
    # indexCells = 'C4' through 'C + str(numRows + 3)'.
    indexColAlignment = Alignment(horizontal = 'left',
                                  vertical   = 'center'
                                  )
    indexColFont = Font(size = 12,
                        bold = True
                        )
    for rowNum in range(4, numRows + 4):
        workSheet['C' + str(rowNum)].alignment = indexColAlignment
        workSheet['C' + str(rowNum)].border    = cellBorder
        workSheet['C' + str(rowNum)].font      = indexColFont
        # Fill index column with appropriate clors
        if workSheet['C' + str(rowNum)].value == 'Marketplace' or workSheet['C' + str(rowNum)].value == 'Owned':
            workSheet['C' + str(rowNum)].fill = PatternFill(fill_type   = 'solid',
                                                            start_color = '9BC2E6',
                                                            end_color   = '9BC2E6'
                                                            )

    # The data starts from row 4, column .
    # dataCells = 'D4' through "chr(lastCol) + str(numRows + 3)"
    if debug_flag == 1:
        print('Setting data cells format.');
    dataCellFont      = Font(size = 12)
    for rowNum in range(4, numRows + 4):
        for colLetter in range(ord('D'), lastCol + ord('A')):
            workSheet[chr(colLetter) + str(rowNum)].border = cellBorder
            workSheet[chr(colLetter) + str(rowNum)].font   = dataCellFont
            if workSheet['C' + str(rowNum)].value == 'Marketplace' or workSheet['C' + str(rowNum)].value == 'Owned':
                workSheet[chr(colLetter) + str(rowNum)].fill = PatternFill(fill_type   = 'solid',
                                                                           start_color = '9BC2E6',
                                                                           end_color   = '9BC2E6'
                                                                           );

    # Set filler column widths.
    workSheet.column_dimensions['A'].width = 3
    workSheet.column_dimensions['B'].width = 3
    workSheet.column_dimensions[chr(lastCol + ord('A'))].width = 3

    # Set index column widths.
    colNumber = 2
    workSheet.column_dimensions[chr(colNumber + ord('A'))].width = getIndexWidth(dfName) + 1

    # Set data cell widths.
    # The '+ 1' factor is to account for width differences between MacOS and Windows.
    colNumber = 3
    for columnWidth in getColumnWidths(dfName):
        workSheet.column_dimensions[chr(colNumber + ord('A'))].width = columnWidth + 1
        colNumber += 1;

    if debug_flag == 1:
        print("Closing xlsxWriter and saving Excel file.");
    xlsxWorkbook.save(XLSXFILE);

    # Closing xlsxWriter and saving Excel file.
    if debug_flag == 1:
        print('### End: stylizeAdditionalWorksheet function ###');

def genBaseDataWorksheet(debug_flag, fyYyNum, numberHoursWorkedPerDay, holidaysPerMonth, numberWorkingDaysPerMonth, XLSXFILE):
    if debug_flag == 1:
        print('### Start: genBaseDataWorksheet function ###');

    ################### Create the content of the work-sheet ######################

    hrsWorkedPerAgent = OrderedDict()
    hrsWorkedPerAgent['Feb'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Feb'])
    hrsWorkedPerAgent['Mar'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Mar'])
    hrsWorkedPerAgent['Apr'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Apr'])
    hrsWorkedPerAgent['May'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['May'])
    hrsWorkedPerAgent['Jun'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Jun'])
    hrsWorkedPerAgent['Jul'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Jul'])
    hrsWorkedPerAgent['Aug'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Aug'])
    hrsWorkedPerAgent['Sep'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Sep'])
    hrsWorkedPerAgent['Oct'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Oct'])
    hrsWorkedPerAgent['Nov'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Nov'])
    hrsWorkedPerAgent['Dec'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Dec'])
    hrsWorkedPerAgent['Jan'] = numberHoursWorkedPerDay * int(numberWorkingDaysPerMonth['Jan'])

    forecastDfColNames = list(holidaysPerMonth)
    for iVal in range(0, 6):
        forecastDfColNames.append('FY' + str(fyYyNum + iVal))

    forecastDf = pd.DataFrame(index = ['Number of hours worked per day',
                                       'Holidays',
                                       'Number of working days',
                                       'Hours Worked Per Agent (Excluding Availability)'
                                       ],
                              columns = forecastDfColNames
                              )
    for colName in holidaysPerMonth.keys():
        forecastDf[colName] = [numberHoursWorkedPerDay,
                               holidaysPerMonth[colName],
                               numberWorkingDaysPerMonth[colName],
                               hrsWorkedPerAgent[colName]
                               ];
    for iVal in range(0, 6):
        forecastDf['FY' + str(fyYyNum + iVal)] = [numberHoursWorkedPerDay,
                                                  sum(holidaysPerMonth.values()),
                                                  sum(numberWorkingDaysPerMonth.values()),
                                                  sum(hrsWorkedPerAgent.values())
                                                  ]

    sheetName = 'Base Data'

    # Create a pandas xlsx writer using XlsxWriter as the engine.
    xlsxFile = pd.ExcelWriter(XLSXFILE,
                              engine = 'xlsxwriter'
                              )

    # Clear all header formats
    pd.formats.format.header_style = None

    # Convert the final data frame to an XlsxWriter excel object.
    # startrow and startcol need to be 0-indexed.
    forecastDf.to_excel(xlsxFile,
                        sheet_name = sheetName,
                        startcol = 2,
                        startrow = 2,
                        index = True
                        )

    ####################### Stylize the work-sheet ##########################

    # Stylize the work-sheet
    stylizeBaseDataWorksheet(debug_flag,
                             forecastDf,
                             sheetName,
                             xlsxFile
                             )

    if debug_flag == 1:
        print('### End: genBaseDataWorksheet function ###');

def genAttrPivotTableWorksheet(debug_flag, yearNum, holidaysPerMonth, XLSXFILE):
    if debug_flag == 1:
        print('### Start: genAttrPivotTableWorksheet function ###');

    ################### Create the content of the work-sheet ######################

    Attr1Dict = {}
    Attr2Dict = {}
    Attr3Dict = {}
    Attr4Dict = {}
    Attr5Dict = {}
    Attr6Dict = {}
    forecastDfColNames = list(holidaysPerMonth)
    forecastDfColNames.append('FY ' + str(yearNum + 1) + ' (YTD)')

    forecastDf = pd.DataFrame(index = ['<Attr 1 Val>',
                                       '<Attr 2 Val>',
                                       '<Attr 3 Val>',
                                       '<Attr 4 Val>',
                                       '<Attr 5 Val>',
                                       '<Attr 6 Val>'
                                       ],
                              columns = forecastDfColNames
                              )

    # Connecting to Teradata.
    if debug_flag == 1:
        print('Connecting to Teradata.');
    udaExec = teradata.UdaExec()
    with udaExec.connect("${dataSourceName}") as session:
        cursor = session.cursor()
        monthDict = {v: k for k, v in enumerate(calendar.month_abbr)}
        for colName in list(holidaysPerMonth):
            monthVal = monthDict[colName]
            if colName != 'Jan':
                yearVal = yearNum;
            else:
                yearVal = yearNum + 1;
            _, numDays = calendar.monthrange(yearVal, monthVal)
            if debug_flag == 1:
                print('Querying for the month of ' + colName + ' ' + str(yearVal) + ':');
            startDate = date(yearVal, monthVal, 1).strftime('%Y-%m-%d')
            endDate   = date(yearVal, monthVal, numDays).strftime('%Y-%m-%d')
            subQueryBegin = 'SELECT COUNT(*) AS Total FROM ' + \
                             TDSBOXNAME + '.' + TDVIEWNAME
            subQueryEnd   = ' AND CAST(<Date Attr> AS DATE) >= \'' + \
                            startDate + \
                            '\' AND CAST(<Date Attr> AS DATE) <= \'' + \
                            endDate + '\');'
            # Attr 1 - Attr 1 Val
            attr1SubQuery      = ' WHERE (<Attr 1 Name> = \'<Attr 1 Val>\''
            attr1QueryResults  = subQueryBegin + attr1SubQuery + subQueryEnd
            attr1QueryResultDf = pd.read_sql(attr1QueryResults,
                                             session
                                             );
            Attr1Dict[colName] = int(attr1QueryResultDf['Total'])
            # Attr 2 - Attr 2 Val
            attr2SubQuery = ' AND <Attr 2 Name> = \'<Attr 2 Val>\''
            attr2QueryResults = subQueryBegin + attr2SubQuery + attr2SubQuery + subQueryEnd
            attr2QueryResultDf = pd.read_sql(attr2QueryResults,
                                             session
                                             );
            Attr2Dict[colName] = int(attr2QueryResults['Total'])
            # Attr 3 - Attr 3 Val
            attr3SubQuery = ' AND <Attr 3 Name> = \'<Attr 3 Val>\''
            attr3QueryResults = subQueryBegin + attr3SubQuery + attr3SubQuery + subQueryEnd
            attr3QueryResultDf = pd.read_sql(attr3QueryResults,
                                             session
                                             );
            Attr3Dict[colName] = int(attr3QueryResults['Total'])
            # Attr 4 - Attr 4 Val
            attr4SubQuery      = ' WHERE (<Attr 4 Name> = \'<Attr 4 Val>\''
            attr4QueryResults  = subQueryBegin + attr4SubQuery + subQueryEnd
            attr4QueryResultDf = pd.read_sql(attr4QueryResults,
                                             session
                                             );
            Attr4Dict[colName] = int(attr4QueryResultDf['Total'])
            # Attr 5 - Attr 5 Val
            attr5SubQuery = ' AND <Attr 5 Name> = \'<Attr 5 Val>\''
            attr5QueryResults = subQueryBegin + attr5SubQuery + attr5SubQuery + subQueryEnd
            attr5QueryResultDf = pd.read_sql(attr5QueryResults,
                                             session
                                             );
            Attr5Dict[colName] = int(attr5QueryResults['Total'])
            # Attr 6 - Attr 6 Val
            attr6SubQuery = ' AND <Attr 6 Name> = \'<Attr 6 Val>\''
            attr6QueryResults = subQueryBegin + attr6SubQuery + attr6SubQuery + subQueryEnd
            attr6QueryResultDf = pd.read_sql(attr6QueryResults,
                                             session
                                             );
            Attr6Dict[colName] = int(attr6QueryResults['Total'])
        # Closing Teradata connection.
        if debug_flag == 1:
            print('Closing the Teradata connection.');
        cursor.close();

    for colName in holidaysPerMonth.keys():
        forecastDf[colName] = [Attr1Dict[colName],
                               Attr2Dict[colName],
                               Attr3Dict[colName],
                               Attr4Dict[colName],
                               Attr5Dict[colName],
                               Attr6Dict[colName]
                               ]

    forecastDf['FY ' + str(yearNum + 1) + ' (YTD)'] = [sum(Attr1Dict.values()),
                                                       sum(Attr2Dict.values()),
                                                       sum(Attr3Dict.values()),
                                                       sum(Attr4Dict.values()),
                                                       sum(Attr5Dict.values()),
                                                       sum(Attr6Dict.values())
                                                       ]

    # Create a pandas xlsx writer using openpyxl as the engine.
    xlsxWorkbook = load_workbook(XLSXFILE)
    xlsxFile = pd.ExcelWriter(XLSXFILE,
                              engine = 'openpyxl'
                              )
    xlsxFile.book = xlsxWorkbook

    # Clear all header formats.
    pd.formats.format.header_style = None

    # Set the sheet name.
    sheetName = '<Attr Name> - Pivot Table'

    # Convert the final data frame to an XlsxWriter excel object.
    # startrow and startcol need to be 0-indexed.
    forecastDf.to_excel(xlsxFile,
                        sheet_name = sheetName,
                        startcol = 2,
                        startrow = 2,
                        index = True
                        )
    xlsxFile.save();

    # Open the updated XLSX using openpyxl.
    xlsxWorkbook = load_workbook(filename = XLSXFILE)
    # workSheet = xlsxWorkbook.get_sheet_by_name(sheetName)
    # for cellObject in workSheet.rows[8]:
        
    # Stylize the work-sheet.
    # The work-sheet gets updated and saved).
    stylizeAdditionalWorksheet(debug_flag,
                               forecastDf,
                               XLSXFILE,
                               xlsxWorkbook,
                               sheetName
                               )

    if debug_flag == 1:
        print('### End: genAttrPivotTableWorksheet function ###');

